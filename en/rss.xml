<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US">
  <id>https://zhul.in</id>
  <title>竹林里有冰的博客</title>
  <updated>2025-11-11T00:00:00.000Z</updated>
  <subtitle>这里是我的个人博客，专注于分享技术干货、学习心得和生活感悟，涵盖前端开发、后端架构、人工智能、编程技巧、工具推荐等多个领域。我持续更新原创内容，记录成长过程中的思考与收获，致力于帮助更多开发者和技术爱好者提升技能、拓展视野。如果你对技术、产品和互联网行业感兴趣，相信你能在这里找到有价值的知识与灵感。</subtitle>
  <rights>© 2025 竹林里有冰</rights>
  <link href="https://zhul.in/en/rss.xml" rel="self" type="application/atom+xml"></link>
  <link href="https://zhul.in" rel="alternate"></link>
  <author>
    <name>竹林里有冰</name>
    <email>zhullyb@outlook.com</email>
    <uri>https://zhul.in</uri>
  </author>
  <icon>https://static.031130.xyz/avatar.webp</icon>
  <logo>https://static.031130.xyz/avatar.webp</logo>
  <entry>
    <id>https://zhul.in/2025/11/11/dns-cold-start-dilemma/</id>
    <title>DNS Cold Start: The &quot;Stone of Sisyphus&quot; for Small Sites</title>
    <updated>2025-11-11T00:00:00.000Z</updated>
    <link href="https://zhul.in/2025/11/11/dns-cold-start-dilemma/" rel="alternate"></link>
    <content type="html">
      <![CDATA[<p>When we talk about website performance, we usually focus on front-end rendering, lazy loading of resources, server response time (TTFB), and so on. However, before the user's browser can even begin to request content, there is a crucial part that is rarely mentioned in performance optimization: DNS resolution. For obscure small sites, a "DNS Cache Miss," or what I call a "DNS Cold Start," can become an unavoidable performance bottleneck. This is the "Stone of Sisyphus" mentioned in the title.</p>
<h2>The Myth's Metaphor: The Long Journey of DNS Resolution</h2>
<p>To understand the weight of this "stone," we must review the complete path of DNS resolution. This isn't a simple lookup, but a global relay race:</p>
<ol>
<li><strong>Starting Point: Public DNS Server</strong> — A user sends a request, and the public DNS server tries to find the answer in its cache.</li>
<li><strong>The First "Push": Root Servers</strong> — Cache Miss. The public DNS server is directed to one of the 13 root server groups worldwide.</li>
<li><strong>The Second Leg: TLD Servers</strong> — The root server points to the Top-Level Domain (TLD) server for the specific suffix (like <code>.com</code>).</li>
<li><strong>The Third Leg: Authoritative Servers</strong> — The TLD server points to the domain's final "steward"—the Authoritative DNS Server.</li>
<li><strong>The Finish Line:</strong> The authoritative server returns the final IP address, which is then returned to the user by the public DNS server.</li>
</ol>
<pre><code class="language-mermaid">sequenceDiagram
    participant User as User/Browser
    participant Local as Local DNS&#x3C;br>Recursive Resolver
    participant Root as Root Server
    participant TLD as TLD Server&#x3C;br>(.com, .org, etc.)
    participant Auth as Authoritative DNS Server

    Note over User,Auth: Full DNS Recursive Query Flow

    User->>Local: 1. Query domain&#x3C;br>[www.example.com](https://www.example.com)
    Note over Local: Check cache&#x3C;br>Record not found

    Local->>Root: 2. Query for .com TLD server
    Root-->>Local: 3. Return .com TLD server address

    Local->>TLD: 4. Query for example.com authoritative server
    TLD-->>Local: 5. Return example.com authoritative server address

    Local->>Auth: 6. Query for [www.example.com](https://www.example.com) A record
    Auth-->>Local: 7. Return IP address (e.g., 1.1.1.1)

    Note over Local: Cache result&#x3C;br>(based on TTL)

    Local-->>User: 8. Return final IP address

    Note over User,Auth: Subsequent process
    User->>Auth: 9. Establish TCP connection with IP&#x3C;br>Start HTTP request
</code></pre>
<p>For a <strong>first-time</strong> or <strong>long-unvisited</strong> request, this process means at least 4 network round-trips (RTT), and even more in cases involving CNAMEs. For large websites with perfect caching, this stone may have already been pushed to the hilltop by someone else. But for a small site, it is always at the foot of the hill, waiting for its Sisyphus.</p>
<h2>A Multiverse: The Mirror Maze of Anycast</h2>
<p>"Since the cost of a DNS cold start is so high, can I use a script to periodically visit my own site to 'warm up' the public DNS cache in advance?" — This was a solution I once envisioned.</p>
<p>However, this idea is often futile under the Anycast architecture of the modern internet.</p>
<p>The core concept of Anycast is: the same IP address exists at multiple global nodes simultaneously, and user requests are routed to the "closest" or "most optimal network path" node.</p>
<p>This means that public DNS servers like Google DNS (8.8.8.8), Cloudflare DNS (1.1.1.1), AliDNS (223.5.5.5), and Tencent DNS (119.29.29.29) are not backed by a single, centralized server, but a cluster of nodes distributed worldwide, routed dynamically.</p>
<p>Thus, the problem arises:</p>
<ul>
<li>The pre-warming script I run in Shanghai might hit the Shanghai node of 223.5.5.5;</li>
<li>But a visitor from Beijing will be routed to the Beijing node of 223.5.5.5;</li>
<li>The caches of these two nodes are <strong>independent and not shared with each other.</strong></li>
</ul>
<p>From a webmaster's perspective, the DNS cache is no longer a predictable entity but has splintered into a "mirror maze" of geographically isolated, ever-changing fragments.</p>
<p>Each visitor is at the base of a different hill, pushing their own stone, as if there are thousands of Sisyphuses in the world, walking their own paths alone.</p>
<h2>Uncontrollable Caching and the "Normalization" of Cold Starts</h2>
<p>This also explains why even if a small site is visited regularly by a script, real visitors might still experience significant DNS latency. Because "pre-warming" is only locally effective—it warms the cache of one Anycast node, not the entire network. And when the TTL expires or the cache is cleared by the public DNS server's algorithm (like LRU), this warmth quietly dissipates.</p>
<p>From a macro perspective, this traps "low-traffic sites" in a kind of fatalistic loop:</p>
<ol>
<li>Due to low traffic, cache hits are rare;</li>
<li>Due to cache misses, resolution time is high;</li>
<li>Due to high resolution time, first-paint performance is poor, and users visit less;</li>
<li>Due to fewer user visits, the cache is even harder to hit.</li>
</ol>
<p>The cold start is no longer an occasional "accident," but a passive "new normal."</p>
<h2>Can We Make the Stone Lighter? — Strategies to Mitigate Cold Start Impact</h2>
<p>The predicament of Sisyphus seems unsolvable, but we are not entirely powerless. While we cannot completely eliminate the DNS cold start, we can significantly reduce the weight of this stone through a series of strategies and shorten the time it takes to be pushed back up the hill after each time it rolls down.</p>
<h3>The Art of the Trade-off: Adjusting DNS TTL (Time-To-Live)</h3>
<p>TTL (Time-To-Live) is a critical value in a DNS record. It tells recursive resolvers (like public DNS, local caches) how long they can cache a record, although they might still be evicted by an LRU algorithm.</p>
<p>Lengthening the TTL can effectively increase the cache hit rate, reduce DNS cold starts, and keep Sisyphus's stone at the top of the hill for as long as possible.</p>
<p>But lengthening the TTL comes at the cost of flexibility: if you need to change the IP address for your domain for any reason, an overly long TTL might cause visitors to retrieve a stale IP address for a long time.</p>
<h3>Choosing a Faster "Messenger": Using the Right Authoritative DNS Server</h3>
<p>The "last mile" of DNS resolution—the time it takes to get from the public DNS server to your authoritative DNS server—is equally critical. If the Nameserver service your domain uses responds slowly, has few global nodes, or is too far from the public DNS server the visitor is querying, then the entire resolution chain will still be slowed down by this final link, even if the user's public DNS node is nearby.</p>
<p>If I were writing this blog post in English, I would just say to switch your Nameserver to a top-tier provider like Cloudflare or Google, and be done with it. These large companies offer free authoritative DNS hosting, have numerous nodes around the world, and are very professional and trustworthy in this regard.</p>
<p>But I am currently using Simplified Chinese. According to my blog's statistics, most of my readers are from mainland China, and the public DNS servers they query are most likely also deployed in mainland China. Whereas Cloudflare/Google Cloud DNS have no authoritative DNS server nodes in mainland China, which will slow things down. So <strong>if your visitors are primarily from mainland China, you might want to try AliYun (Alibaba Cloud) or Dnspod</strong>. Their main authoritative DNS server nodes are within mainland China, which, in theory, can reduce the communication time between the public DNS server and the authoritative DNS server.</p>
<h2>Conclusion: The Stone Pusher</h2>
<p>There has never been a perfect solution to the problem of DNS cold starts. It's like a fated "poetry of latency" within the internet's architecture—each visitor starts from their own network topology, pushing their own stone step-by-step along an invisible path, until they reach the summit of your server, in exchange for the first pixel lighting up on their screen.</p>
<p>For small sites, this may be the weight of destiny; but to understand it, optimize it, and monitor it, is how we, on this long uphill road, polish the stone's edges to make them smoother.</p>
<h2>See Also</h2>
<ul>
<li><a href="https://developers.google.com/speed/public-dns/docs/performance">Performance Benefits | Public DNS | Google for Developers</a></li>
<li><a href="https://falconcloud.ae/about/blog/how-do-dns-queries-affect-website-latency/">How do DNS queries affect website latency? - falconcloud.ae</a></li>
</ul>]]>
    </content>
    <published>2025-11-11T00:00:00.000Z</published>
    <author>
      <name>竹林里有冰</name>
      <email>zhullyb@outlook.com</email>
      <uri>https://zhul.in</uri>
    </author>
    <category term="DNS"></category>
    <category term="Network"></category>
  </entry>
  <entry>
    <id>https://zhul.in/2025/11/05/http-2-server-push-is-practically-obsolete/</id>
    <title>HTTP/2 Server Push Has Effectively &quot;Died&quot; - I Miss It</title>
    <updated>2025-11-05T00:00:00.000Z</updated>
    <link href="https://zhul.in/2025/11/05/http-2-server-push-is-practically-obsolete/" rel="alternate"></link>
    <content type="html">
      <![CDATA[<p>I've been refactoring my blog recently, and while preparing for autumn recruitment and memorizing technical concepts, I came across HTTP/2 server push. I then attempted to configure HTTP/2 server push for my blog during deployment to further optimize first-screen loading speed.</p>
<h2>Why HTTP/2 Server Push Could Improve First-Screen Loading Speed</h2>
<p>As shown in the diagram below, in traditional HTTP/1.1, the browser first downloads <code>index.html</code> and completes the initial parsing, then retrieves the URLs for CSS/JS resources from the parsed data before making a second round of requests. After establishing TCP/TLS connections, it requires <strong>at least two RTTs to retrieve</strong> all the resources needed to fully render the page.</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant Browser
    participant Server

    Browser->>Server: GET /index.html
    Server-->>Browser: 200 OK + HTML
    Browser->>Server: GET /style.css
    Browser->>Server: GET /app.js
    Server-->>Browser: 200 OK + CSS
    Server-->>Browser: 200 OK + JS

    Note over Browser: Browser must wait for HTML to download&#x3C;br/>and parse before making subsequent requests,&#x3C;br/>increasing round-trip latency (RTT)
</code></pre>
<p>In HTTP/2's vision, the process would look like the diagram below. When the browser requests index.html, the server can simultaneously push CSS/JS resources to the client. This way, after establishing the TCP/TLS connection, it only needs <strong>one RTT</strong> to retrieve all resources needed for page rendering.</p>
<pre><code class="language-mermaid">sequenceDiagram
    participant Browser
    participant Server

    Browser->>Server: GET /index.html
    Server-->>Browser: 200 OK + HTML
    Server-->>Browser: PUSH_PROMISE /style.css
    Server-->>Browser: PUSH_PROMISE /app.js
    Server-->>Browser: (Push) style.css + app.js content

    Note over Browser: Browser receives proactive resource push&#x3C;br/>Reduces request rounds and first-screen latency
</code></pre>
<p>To minimize subsequent requests in HTTP/1.1, front-end developers have tried numerous optimization techniques. As Sukka mentioned in "<a href="https://blog.skk.moe/post/http2-server-push/">Static Resource Delivery Optimization: HTTP/2 and Server Push</a>":</p>
<blockquote>
<p>The concepts of critical resources, critical rendering path, and critical request chain have existed for a long time. Asynchronous resource loading is old news: lazy-loading images, videos, iframes, and even lazy-loading CSS, JS, DOM, and lazy execution of functions. However, the approach to critical resource delivery hasn't changed much.</p>
</blockquote>
<p>HTTP/2 Server Push created a new resource delivery paradigm. CSS/JS and other resources don't need to be delivered along with HTML to reach the client within one RTT, and these resources can be cached by the browser without being constrained by HTML's shorter TTL.</p>
<h2>Initial Solution</h2>
<p>Having understood the advantages of HTTP/2 server push, I prepared to implement the optimization. My blog is purely static, using DNS for traffic splitting between domestic and international visitors: domestic traffic accesses a DMIT VPS with cmin2/9929 network optimization, served through Caddy; international traffic goes directly to Vercel, leveraging Amazon's CDN for global edge acceleration. The network architecture looks roughly like this:</p>
<pre><code class="language-mermaid">graph TD
    A[Blog Visitors] --> B[Initiate DNS Resolution]
    B --> C[DNSPod Service]
    C -->|Domestic Visitors: Smart Routing| D[Network-Optimized VPS]
    C -->|International Visitors: Smart Routing| E[Vercel Platform]
    D --> F[Caddy]
    F --> G[Return Blog Content to Domestic Visitors]
    E --> H[Return Blog Content to International Visitors]
</code></pre>
<p>Caddy can implement HTTP/2 server push through the <code>http.handlers.push</code> module. We can write simple push logic in the Caddyfile—no problem there. However, Vercel doesn't provide configuration options for HTTP/2 server push. Fortunately, since I have a static blog with low platform dependency, I considered migrating to Cloudflare Workers, <a href="https://brianli.com/cloudflare-workers-sites-http2-server-push/">which developers implemented five years ago</a>.</p>
<h2>Client Support Status</h2>
<p>Historically, mainstream browser engines (Chrome/Chromium, Firefox, Edge, Safari) widely supported server push technology.</p>
<p>In November 2020, Google announced <a href="https://groups.google.com/a/chromium.org/g/blink-dev/c/K3rYLvmQUBY/m/vOWBKZGoAQAJ">plans to remove server push functionality from Chrome's HTTP/2 and gQUIC (which later evolved into HTTP/3) implementations</a>.</p>
<p>In October 2022, Google announced <a href="https://developer.chrome.com/blog/removing-push/">plans to remove server push from Chrome</a>, citing poor real-world performance, low adoption rates, and better alternatives. Chrome 106 became the first version to disable server push by default.</p>
<p>On October 29, 2024, Mozilla released Firefox 132, <a href="https://www.firefox.com/en-US/firefox/132.0/releasenotes/">removing support for HTTP/2 server push due to "compatibility issues with multiple websites."</a></p>
<p>With this, mainstream browser support for HTTP/2 Server Push has completely ended. From initially being viewed as an innovative feature to "reduce round-trip latency and optimize first-screen loading," to its eventual complete deprecation, HTTP/2 push's lifecycle lasted only a few short years, becoming an important experiment in web performance optimization history.</p>
<h2>Alternative Solutions</h2>
<h3>1. HTTP 103 Early Hints</h3>
<p>103 Early Hints is the most direct "successor" to server push. It's an informational HTTP status code that allows the server to send an "early hint" response with Link headers before generating the complete HTML response (e.g., status code 200 OK).</p>
<p>This Link header can tell the browser: "Hey, I'm still preparing the main course (HTML), but you can start preparing the side dishes (CSS, JS) first." This way, the browser can utilize the server's "thinking time" to preemptively download critical resources or warm up connections to required origins, significantly reducing first-screen rendering time.</p>
<p>Compared to server push:</p>
<ul>
<li>Decision authority lies with the client: Early Hints are just "hints." The browser can decide whether to adopt them based on its own cache status, network conditions, and other factors. This solves server push's biggest pain point—servers cannot know about client caches, leading to redundant resource pushes.</li>
<li>Better compatibility: It's a lighter mechanism that's easier for intermediary proxy servers to understand and relay.</li>
</ul>
<p>103 Early Hints is very meaningful for dynamic blogs. Before backend computation, the 103 response can inform the browser about needed resources, allowing the browser to retrieve other resources first while waiting for the backend to return the final HTML. However, <strong>for static blogs</strong> like mine where <strong>everything is pre-built</strong>, it has <strong>absolutely no meaning</strong>. The gateway has enough time to send the 103 response that it could just directly send the HTML instead.</p>
<h3>2. Resource Hints: Preload &#x26; Prefetch</h3>
<p>Even before server push was deprecated, resource hints implemented through <code>&#x3C;link></code> tags were already common tools for front-end performance optimization. They declare resource loading hints in HTML, with the browser leading the entire process.</p>
<ul>
<li><code>&#x3C;link rel="preload"></code>: Tells the browser about resources that the current page will definitely use, requesting immediate high-priority loading without execution. For example, font files hidden deep in CSS or first-screen images dynamically loaded by JS. Through preload, you can ensure these critical resources are discovered and downloaded early, avoiding render blocking.</li>
<li><code>&#x3C;link rel="prefetch"></code>: Tells the browser about pages or resources the user might access in the future, requesting low-priority background downloads during browser idle time. For example, prefetching article page resources that users are most likely to click on the article list page, achieving near-"instant" navigation experience.</li>
</ul>
<p>Preload and Prefetch completely hand over resource loading control to developers and browsers. Through declarative methods, they allow fine-grained management of resource loading priority and timing. They are currently the most mature and widely applied resource preloading solutions, but still <strong>cannot escape the curse of 2 RTTs</strong>.</p>
<h2>Epilogue: An Elegy for an Idealist</h2>
<p>In the end, I couldn't configure HTTP/2 server push for my blog.</p>
<p><strong>HTTP/2 Server Push has effectively "died." I miss it.</strong></p>
<p>In an ideal model, when the browser requests HTML, the server conveniently pushes the CSS and JS needed for rendering along with it, cleanly compressing what would originally be at least two round trips (RTT) into one. This is such a direct, such an elegant solution—almost the "silver bullet" that front-end engineers dream of when facing first-screen rendering latency issues. Behind it lies an ambitious spirit: attempting to thoroughly solve the "critical request chain" latency problem from the server side, once and for all.</p>
<p>But the Web world is ultimately not an ideal laboratory. It's full of caches, returning users, and all kinds of network environments.</p>
<p>The greatest charm of server push lies in its "proactivity," and its greatest regret also stems precisely from this "proactivity." It cannot know whether the browser's cache already quietly holds that style.css file it's preparing to enthusiastically push. For the sake of the ultimate experience for a small portion of first-time visitors, it might waste precious bandwidth for more returning users.</p>
<p>The Web's evolution ultimately chose a more prudent path with a more collaborative spirit. It returned decision-making authority to the browser, which knows the situation best. The entire interaction changed from the server's "<strong>I push to you</strong>" to the server's "<strong>I suggest you fetch</strong>," with the browser making its own judgment. This may not be romantic enough, not extreme enough, but it's more universal and more robust.</p>
<p>So, I still miss that ambitious Server Push. It represented a pure pursuit of ultimate performance, a beautiful technical idealism. Although it has quietly faded from the historical stage, the dream about "speed" it pointed toward has long been inherited by 103 Early Hints and preload in a more mature, more balanced way.</p>
<h2>See Also</h2>
<ul>
<li><a href="https://developer.chrome.com/blog/removing-push">Remove HTTP/2 Server Push from Chrome  |  Blog  |  Chrome for Developers</a></li>
<li><a href="https://en.wikipedia.org/wiki/HTTP/2_Server_Push">HTTP/2 Server Push - Wikipedia</a></li>
<li><a href="https://blog.skk.moe/post/http2-server-push/">静态资源递送优化：HTTP/2 和 Server Push | Sukka's Blog</a></li>
<li><a href="https://caddyserver.com/docs/modules/http.handlers.push">Module http.handlers.push - Caddy Documentation</a></li>
<li><a href="https://brianli.com/cloudflare-workers-sites-http2-server-push/">How to Configure HTTP/2 Server Push on Cloudflare Workers Sites</a></li>
<li><a href="https://groups.google.com/a/chromium.org/g/blink-dev/c/K3rYLvmQUBY/m/vOWBKZGoAQAJ">Intent to Remove: HTTP/2 and gQUIC server push</a></li>
</ul>]]>
    </content>
    <published>2025-11-05T00:00:00.000Z</published>
    <author>
      <name>竹林里有冰</name>
      <email>zhullyb@outlook.com</email>
      <uri>https://zhul.in</uri>
    </author>
    <category term="HTTP"></category>
    <category term="Caddy"></category>
    <category term="Network"></category>
    <category term="HTML"></category>
    <category term="Vercel"></category>
  </entry>
  <entry>
    <id>https://zhul.in/2025/10/20/nuxt-content-v3-z-array-query-challenge/</id>
    <title>Array Field Filtering Challenges and Performance Optimization in Nuxt Content v3</title>
    <updated>2025-10-20T13:52:59.000Z</updated>
    <link href="https://zhul.in/2025/10/20/nuxt-content-v3-z-array-query-challenge/" rel="alternate"></link>
    <content type="html">
      <![CDATA[<p>Nuxt Content is a powerful module in the Nuxt ecosystem for handling Markdown, YAML, and other content types. Recently, while migrating my blog from Hexo to <strong>Nuxt v4 + Nuxt Content v3</strong>, I encountered a tricky issue: the v3 default query API <strong>does not directly provide</strong> support for "contains" (<code>$contains</code>) operations on array fields.</p>
<p>For example, here's the Front Matter of the blog post I'm currently writing:</p>
<pre><code class="language-markdown">---
title: Array Field Filtering Challenges in Nuxt Content v3
date: 2025-10-20 21:52:59
sticky:
tags:
- Nuxt
- Nuxt Content
- JavaScript
---
</code></pre>
<p>My goal was to create a <strong>Tag page</strong> that lists all articles containing a specific tag (e.g., 'Nuxt').</p>
<h2>The Convenience of v2 and the Limitations of v3</h2>
<p>In Nuxt Content v2, data was stored based on the file system, and the query approach abstracted file content with a syntax similar to <strong>MongoDB's JSON document queries</strong>. We could easily use the <code>$contains</code> method to retrieve all articles with the "Nuxt" tag:</p>
<pre><code class="language-typescript">const tag = decodeURIComponent(route.params.tag as string)

const articles = await queryContent('posts')
  .where({ tags: { $contains: tag } })  // ✅ MongoDB-style queries in v2
  .find()
</code></pre>
<p>However, when using <strong>Nuxt Content v3's <code>queryCollection</code> API</strong>, we naturally try to use the <code>.where()</code> method for filtering:</p>
<pre><code class="language-typescript">const tag = decodeURIComponent(route.params.tag as string)

const { data } = await useAsyncData(`tag-${tag}`, () =>
    queryCollection('posts')
        .where(tag, 'in', 'tags')  // ❌ This will error because the first parameter must be a field name
        .order('date', 'DESC')
        .select('title', 'date', 'path', 'tags')
        .all()
)
</code></pre>
<p>Unfortunately, this approach doesn't work. The <code>.where()</code> method signature requires the field name as the first parameter: <code>where(field: keyof Collection | string, operator: SqlOperator, value?: unknown)</code>.</p>
<p>Since Nuxt Content v3 <strong>uses SQLite as its underlying local database</strong>, all queries must follow SQL-like syntax. If the design doesn't provide built-in operators for array fields (such as an SQL equivalent of <code>$contains</code>), the eventual solution often feels somewhat "awkward."</p>
<h2>Initial Implementation: Sacrificing Performance with "Fetch All"</h2>
<p>Following a "migrate quickly, optimize later" approach, I wrote the following code:</p>
<pre><code class="language-typescript">// Initial implementation: fetch all and filter with JS
const allPosts = (
    await useAsyncData(`tag-${route.params.tag}`, () =>
        queryCollection('posts')
            .order('date', 'DESC')
            .select('title', 'date', 'path', 'tags')
            .all()
    )
).data as Ref&#x3C;Post[]>

const Posts = computed(() => {
    return allPosts.value.filter(post =>
        typeof post.tags?.map === 'function'
            ? post.tags?.includes(decodeURIComponent(route.params.tag as string))
            : false
    )
})
</code></pre>
<p>While this method met the requirements, it came with obvious performance costs: <strong>bloated <code>_payload.json</code> file sizes.</strong></p>
<p>In Nuxt projects, <code>_payload.json</code> stores dynamic data such as <code>useAsyncData</code> results. With the fetch-all approach, <strong>every Tag page</strong> loads a <code>_payload.json</code> containing information for all articles, causing data redundancy. Many tag pages only need data for one or two articles but are forced to load information for all articles, severely impacting performance.</p>
<p><img src="https://static.031130.xyz/uploads/2025/10/20/a748878c03c64.webp" alt="The tags directory occupies 2.9MiB, the largest of all directories"></p>
<p><img src="https://static.031130.xyz/uploads/2025/10/20/8ef786d873da1.webp" alt="_payload.json"></p>
<h2>A Clever Solution: Leveraging SQLite's Storage Characteristics for Optimization</h2>
<p>To reduce the query results returned by <code>useAsyncData</code>, I searched through Nuxt Content's GitHub Discussions and found <a href="https://github.com/nuxt/content/discussions/2955">a "clever" solution proposed during v3.alpha.8</a>.</p>
<p>Since Nuxt Content v3 uses an SQLite database, the <strong><code>tags</code> array originally defined in Front Matter (via <code>z.array()</code>) is ultimately stored as a JSON string</strong> in the database (you can view the exact format in the <code>.nuxt/content/sql_dump.txt</code> file).</p>
<p><img src="https://static.031130.xyz/uploads/2025/10/20/b70036c55bb29.webp" alt="sql_dump.txt"></p>
<p>This means we can leverage SQLite's <strong>string operation</strong> features by using the <strong><code>LIKE</code> operator with wildcards</strong> to perform array containment filtering, essentially querying whether the JSON string contains a specific substring:</p>
<pre><code class="language-typescript">const tag = decodeURIComponent(route.params.tag as string)

const { data } = await useAsyncData(`tag-${route.params.tag}`, () =>
    queryCollection('posts')
        .where('tags', 'LIKE', `%"${tag}"%`)
        .order('date', 'DESC')
        .select('title', 'date', 'path', 'tags')
        .all()
)
</code></pre>
<p>Below are the file sizes after optimization and regeneration - the reduction is quite significant:</p>
<ul>
<li>Tags directory size: 2.9MiB → 1.4MiB</li>
<li>Individual _payload.json size: 23.1KiB → 1.01 KiB</li>
</ul>
<p>Through this method, we successfully pushed the query logic down to the database layer, avoided unnecessary full data transfers, significantly reduced the size of <code>_payload.json</code> in individual directories, and achieved performance optimization.</p>
<p><img src="https://static.031130.xyz/uploads/2025/10/20/007e72e7b476d.webp" alt="Tags directory size reduction"></p>
<p><img src="https://static.031130.xyz/uploads/2025/10/20/17ba3ccbbdf9e.webp" alt="_payload.json"></p>
<h2>See Also</h2>
<p><a href="https://content.nuxt.com/docs/utils/query-collection#wherefield-keyof-collection-string-operator-sqloperator-value-unknown">queryCollection - Nuxt Content</a></p>
<p><a href="https://github.com/nuxt/content/discussions/2955">How do you query <code>z.array()</code> fields (e.g. tags) in the latest nuxt-content module (v3.alpha.8) · nuxt/content · Discussion #2955</a></p>]]>
    </content>
    <published>2025-10-20T13:52:59.000Z</published>
    <author>
      <name>竹林里有冰</name>
      <email>zhullyb@outlook.com</email>
      <uri>https://zhul.in</uri>
    </author>
    <category term="Nuxt"></category>
    <category term="Nuxt Content"></category>
    <category term="JavaScript"></category>
  </entry>
  <entry>
    <id>https://zhul.in/2025/10/16/how-s-mozilla-crlite-going-now/</id>
    <title>Post-OCSP Era: How Browsers Address New Certificate Revocation Challenges</title>
    <updated>2025-10-16T07:38:50.000Z</updated>
    <link href="https://zhul.in/2025/10/16/how-s-mozilla-crlite-going-now/" rel="alternate"></link>
    <content type="html">
      <![CDATA[<p>In August 2023, the CA/Browser Forum passed a vote eliminating the requirement for publicly trusted CAs like Let's Encrypt to maintain OCSP servers.</p>
<p>In July 2024, Let's Encrypt published a <a href="https://letsencrypt.org/2024/07/23/replacing-ocsp-with-crls">blog post</a> disclosing its plans to shut down its OCSP server.</p>
<p>In December of the same year, Let's Encrypt released <a href="https://letsencrypt.org/2024/12/05/ending-ocsp">a timeline for shutting down its OCSP server</a>, with the following key dates:</p>
<ul>
<li>January 30, 2025 - Let's Encrypt will no longer accept new certificate issuance requests containing the OCSP Must-Staple extension, unless your account has previously requested such certificates</li>
<li>May 7, 2025 - Newly issued certificates from Let's Encrypt will include CRL URLs but will no longer contain OCSP URLs, and all new certificate issuance requests with the OCSP Must-Staple extension will be rejected</li>
<li>August 6, 2025 - Let's Encrypt will shut down its OCSP servers</li>
</ul>
<p><strong>Let's Encrypt is the world's largest free SSL certificate authority, and this move marks our gradual transition into the post-OCSP era.</strong></p>
<h2>OCSP's Dilemma: The Trade-off Between Performance and Privacy</h2>
<p>Behind Let's Encrypt's decision lies long-accumulated dissatisfaction with OCSP (Online Certificate Status Protocol). OCSP, as a method for real-time certificate validity queries, had a beautiful initial vision: when a browser accesses a website, it can send a brief request to the <strong>CA's (Certificate Authority's)</strong> OCSP server, asking whether the certificate is still valid. This seemed much more efficient than downloading a massive <strong>CRL (Certificate Revocation List)</strong>.</p>
<p>However, OCSP has exposed numerous flaws in practical application:</p>
<p>First is the <strong>performance issue</strong>. Although individual requests are small, when millions of users simultaneously access websites, OCSP servers must handle massive amounts of real-time queries. This not only creates enormous server pressure for CAs but also increases latency for users accessing websites. If OCSP servers respond slowly or even crash, browsers may interrupt connections due to inability to confirm certificate status, or have to "turn a blind eye" for the sake of user experience, both of which undermine OCSP's security.</p>
<p>More seriously is the <strong>privacy issue</strong>. Each OCSP query essentially reports the user's browsing behavior to the CA. This means the CA can know when a user accessed which website. While OCSP queries themselves don't contain personally identifiable information, by combining this information with data like IP addresses, CAs can completely build profiles of users' browsing habits. For privacy-conscious users and developers, this "silent surveillance" is unacceptable. <strong>Even if CAs intentionally don't retain this information, regional laws may force CAs to collect it.</strong></p>
<p>Furthermore, OCSP has <strong>security flaws</strong> in its design. Due to concerns about connection timeouts affecting user experience, browsers typically default to a soft-fail mechanism: if they cannot connect to the OCSP server, they choose to allow rather than block the connection. Attackers can exploit this by blocking communication between the client and OCSP server, causing queries to always timeout, thus easily bypassing certificate status verification.</p>
<h3>OCSP Stapling</h3>
<p>Based on these flaws, we have the OCSP stapling solution, which <a href="/2024/11/19/firefox-is-the-only-mainstream-brower-doing-online-certificate-revocation-checks/#OCSP-%E8%A3%85%E8%AE%A2-OCSP-stapling">I covered in last year's blog post, feel free to review</a>.</p>
<h3>OCSP Must-Staple</h3>
<p>OCSP Must-Staple is an extension option when applying for SSL certificates. This extension tells the browser: if it recognizes this extension in the certificate, it must not send query requests to the certificate authority, but should obtain the stapled copy during the handshake phase. If a valid copy cannot be obtained, the browser should refuse the connection.</p>
<p>This feature gave browser developers the courage for hard-fail, but before OCSP faded from history, Let's Encrypt seemed to be the only mainstream CA supporting this extension, and this feature was not widely used.</p>
<p><del>I originally didn't want to introduce this feature (because literally no one used it), but considering this thing is about to be buried, let's erect a monument for it on the Chinese internet,</del> for more information, refer to <a href="https://letsencrypt.org/2024/12/05/ending-ocsp#must-staple">Let's Encrypt's blog</a>.</p>
<h2>Chromium's Solution: Taking Only a Ladle from Three Thousand Waters</h2>
<p>OCSP's privacy and performance issues are no secret, and browser vendors have long begun their own explorations. In 2012, Chrome disabled CRLs and OCSP checks by default, turning to its own self-designed certificate verification mechanism.</p>
<p>It's well known that revocation lists can be extremely large. If browsers needed to download and parse a complete global revocation list, it would be a performance disaster (Mozilla's team mentioned in <a href="https://hacks.mozilla.org/2025/08/crlite-fast-private-and-comprehensive-certificate-revocation-checking-in-firefox/">this year's blog</a> that file sizes from downloading 3000 active CRLs would reach 300MB). Through analyzing historical data, the Chromium team discovered that most revoked certificates belong to a few high-risk categories, such as the certificate authority (CA) itself being compromised, or certificates from certain large websites being revoked. Based on this insight, CRLSets adopts the following strategy:</p>
<ol>
<li><strong>Tiered Revocation</strong>: Chromium doesn't download all revoked certificate information, but rather the Google team maintains a streamlined list containing the "most important" revocation information. This list is regularly updated and pushed to users through Chrome browser updates.</li>
<li><strong>Streamlined Efficiency</strong>: This list is very small in size, currently about 600KB. It contains certificates that would cause large-scale security incidents if abused, such as CA intermediate certificates, or certificates from some well-known websites (like Google, Facebook).</li>
<li><strong>Sacrificing Partial Security</strong>: The disadvantage of this approach is also obvious—it cannot cover all certificate revocation situations. For an ordinary website's revoked certificate, CRLSets likely cannot detect it. According to Mozilla's blog this year, CRLSets only contains 1%-2% of unexpired revoked certificate information.</li>
</ol>
<p>While CRLSets is an "imperfect" solution, it has found a balance between performance and usability. It ensures basic security for users accessing mainstream websites while avoiding the performance and privacy overhead brought by OCSP. For Chromium, rather than pursuing an OCSP solution that's difficult to implement perfectly in reality, it's better to concentrate efforts on solving the most urgent security threats.</p>
<h2>Firefox's Solution: From CRLs to CRLite</h2>
<p>Unlike Chromium's "taking only a ladle" strategy, Firefox developers have been searching for a solution that can guarantee comprehensiveness while solving performance issues.</p>
<p>To solve this problem, Mozilla proposed an innovative solution: <strong>CRLite</strong>. CRLite's design philosophy is to use data structures like <strong>hash functions and Bloom filters</strong> to compress massive certificate revocation lists into a <strong>compact, downloadable, and easily locally verifiable format</strong>.</p>
<p>CRLite's working principle can be simply summarized as:</p>
<ol>
<li><strong>Data Compression</strong>: CAs periodically generate lists of all their revoked certificates.</li>
<li><strong>Server Processing</strong>: Mozilla's servers collect these lists and use techniques like cryptographic hash functions and Bloom filters to <strong>encode</strong> all revoked certificate information into a very compact data structure.</li>
<li><strong>Client Verification</strong>: The browser downloads this compressed file, and when accessing a website, it only needs to perform hash calculations on the certificate locally, then query this local file to quickly determine whether the certificate has been revoked.</li>
</ol>
<p>Compared to CRLSets, CRLite's advantage is that it can achieve <strong>comprehensive coverage of all revoked certificates</strong> while maintaining an <strong>extremely small size</strong>. More importantly, it <strong>completes verification entirely locally</strong>, which means the browser <strong>doesn't need to send requests to any third-party servers</strong>, thus completely solving OCSP's privacy problem.</p>
<p>Firefox's current strategy performs incremental updates to CRLite data every 12 hours, with daily downloads of approximately 300KB; it performs a full snapshot sync every 45 days, with downloads of approximately 4MB.</p>
<p>Mozilla has opened their data dashboard, where you can find recent CRLite data sizes: <a href="https://yardstick.mozilla.org/dashboard/snapshot/c1WZrxGkNxdm9oZp7xVvGUEFJCELfApN">https://yardstick.mozilla.org/dashboard/snapshot/c1WZrxGkNxdm9oZp7xVvGUEFJCELfApN</a></p>
<p>Starting with Firefox Desktop version 137 released on April 1, 2025, Firefox began gradually replacing OCSP validation with CRLite; on August 19 of the same year, Firefox Desktop 142 officially deprecated OCSP verification for DV certificates.</p>
<p>CRLite has become the core solution for Firefox's future certificate revocation verification, representing a comprehensive pursuit of performance, privacy, and security.</p>
<h2>Outlook for the Post-OCSP Era</h2>
<p>With major CAs like Let's Encrypt shutting down OCSP services, the OCSP era is rapidly drawing to a close. We can see that browser vendors have already begun exploring more efficient and secure alternative solutions.</p>
<ul>
<li><strong>Chromium</strong>, with its CRLSets solution, has achieved a pragmatic balance between <strong>performance and critical security guarantees</strong>.</li>
<li><strong>Firefox</strong>, through the technological innovation of <strong>CRLite</strong>, attempts to find the optimal solution among <strong>comprehensiveness, privacy, and performance</strong>.</li>
</ul>
<p>What these solutions have in common is: <strong>transforming certificate revocation verification from real-time online queries (OCSP) to localized verification</strong>, thereby avoiding OCSP's inherent performance bottlenecks and privacy risks.</p>
<p>In the future, the certificate revocation ecosystem will no longer rely on a single, centralized OCSP server. Instead, a more diverse, distributed, and intelligent new era is arriving. <strong>OCSP as a technology may gradually be phased out, but the core security problem of "certificate revocation" that it attempted to solve will forever remain a focus of browsers and the network security community.</strong></p>
<h2>See Also</h2>
<ul>
<li><a href="https://hacks.mozilla.org/2025/08/crlite-fast-private-and-comprehensive-certificate-revocation-checking-in-firefox/">CRLite: Fast, private, and comprehensive certificate revocation checking in Firefox - Mozilla Hacks - the Web developer blog</a></li>
<li><a href="https://www.feistyduck.com/newsletter/issue_121_the_slow_death_of_ocsp">The Slow Death of OCSP | Feisty Duck</a></li>
<li><a href="https://github.com/mozilla/crlite">mozilla/crlite: Compact certificate revocation lists for the WebPKI</a></li>
<li><a href="https://letsencrypt.org/2025/08/06/ocsp-service-has-reached-end-of-life">OCSP Service Has Reached End of Life - Let's Encrypt</a></li>
<li><a href="https://letsencrypt.org/2024/12/05/ending-ocsp">Ending OCSP Support in 2025 - Let's Encrypt</a></li>
<li><a href="https://letsencrypt.org/2024/07/23/replacing-ocsp-with-crls">Intent to End OCSP Service - Let's Encrypt</a></li>
<li><a href="https://www.chromium.org/Home/chromium-security/crlsets/">CRLSets - The Chromium Projects</a></li>
<li><a href="https://www.pcworld.com/article/474296/google_chrome_will_no_longer_check_for_revoked_ssl_certificates_online-2.html">Google Chrome Will No Longer Check for Revoked SSL Certificates Online | PCWorld</a></li>
<li><a href="https://www.zdnet.com/article/chrome-does-certificate-revocation-better/">Chrome does certificate revocation better | ZDNET</a></li>
<li><a href="https://www.hats-land.com/WIP/2025-technical-and-analysis-of-mainstream-clientbrowser-certificate-revocation-verification-mechanism.html">主流客户端/浏览器证书吊销验证机制技术对与分析 | 帽之岛, Hat's Land</a></li>
<li><a href="https://blog.gslin.org/archives/2025/02/02/12239/ocsp-%E7%9A%84%E6%B7%A1%E5%87%BA/">OCSP 的淡出… – Gea-Suan Lin's BLOG</a></li>
</ul>]]>
    </content>
    <published>2025-10-16T07:38:50.000Z</published>
    <author>
      <name>竹林里有冰</name>
      <email>zhullyb@outlook.com</email>
      <uri>https://zhul.in</uri>
    </author>
    <category term="SSL"></category>
    <category term="Firefox"></category>
    <category term="Web PKI"></category>
    <category term="OCSP"></category>
    <category term="CRLSets"></category>
    <category term="CRLite"></category>
  </entry>
  <entry>
    <id>https://zhul.in/2025/09/05/first-try-of-github-action-self-hosted-runner/</id>
    <title>First Experience with GitHub Action Self-hosted Runner: Love is Not Easy</title>
    <updated>2025-09-04T21:54:17.000Z</updated>
    <link href="https://zhul.in/2025/09/05/first-try-of-github-action-self-hosted-runner/" rel="alternate"></link>
    <content type="html">
      <![CDATA[<p>In August of this year, a GitHub Organization I'm part of frequently triggered CI during private project development, exhausting the <a href="https://docs.github.com/en/get-started/learning-about-github/githubs-plans#github-free-for-organizations">2,000 minutes of monthly Action quota</a> provided by GitHub for the Free Plan (shared across all private repositories, public repositories don't count). After reviewing the CI workflow setup, which seemed reasonable, I needed to find alternative solutions to provide more generous resources. This led me to explore the <a href="https://docs.github.com/en/actions/concepts/runners/self-hosted-runners">GitHub Action Self-hosted Runner</a> mentioned in the article title.</p>
<p>Compared to GitHub's official runners, Self-hosted Runners offer several key advantages:</p>
<ul>
<li>Unlimited Action runtime for private repositories</li>
<li>Ability to configure more powerful hardware computing power and memory</li>
<li>Access to internal network environments, facilitating communication with intranet/LAN devices</li>
</ul>
<h2>Configuration and Installation</h2>
<p>Since I wasn't sure about the network environment requirements, I directly chose an idle Hong Kong VPS for this test, with specs of 4 cores, 4GB RAM, 80GB disk, and 1Gbps bandwidth. Aside from somewhat lacking disk read/write performance, everything else was maxed out.</p>
<p>The configuration of Self-hosted Runner itself is quite straightforward and clear, following the official guidelines without any issues.</p>
<p><img src="https://static.031130.xyz/uploads/2025/09/05/7c0475cdb1aa9.webp" alt=""></p>
<p>All three mainstream platforms are supported, and if utilized properly, it should cover a range of needs including iPhone app packaging.</p>
<p><img src="https://static.031130.xyz/uploads/2025/09/05/96ff7cb263da1.webp" alt=""></p>
<p>Looking at the runner installation file I received, version 2.328.0, the compressed package is around 220MB and includes built-in node20 and node24 runtime environments, two versions each.</p>
<p><img src="https://static.031130.xyz/uploads/2025/09/05/f775e3bcd2cdc.webp" alt=""></p>
<p><img src="https://static.031130.xyz/uploads/2025/09/05/d0d4fe4611a40.webp" alt=""></p>
<p>After executing config.sh, a svc.sh file appears in the current directory, which can be used to leverage systemd for process management and similar needs.</p>
<p><img src="https://static.031130.xyz/uploads/2025/09/05/43c6b19038def.webp" alt=""></p>
<p>Refreshing the web page again shows that the Self-hosted Runner is now online.</p>
<p><img src="https://static.031130.xyz/uploads/2025/09/05/6dad15beff900.webp" alt=""></p>
<h2>Specifying Actions to Use Your Own Runner</h2>
<p>This step is simple - just change the runs-on field in the original Action's yml file:</p>
<pre><code class="language-diff">jobs:
  run:
+    runs-on: self-hosted
-    runs-on: ubuntu-latest
</code></pre>
<h2>Real-World Testing</h2>
<p>When I excitedly switched the CI workflow from GitHub's official runner to the self-hosted runner, problems quickly surfaced, and this is the main reason I "can't love it." The issues were concentrated in the <code>setup-python</code> GitHub Action Flow, which is maintained by GitHub officially, showing an error that version 3.12 wasn't found.</p>
<p><img src="https://static.031130.xyz/uploads/2025/09/05/1c93947170a85.webp" alt=""></p>
<p>In GitHub's official virtual environment, these Actions prepare the specified version of the development environment for us. For example, <code>uses: actions/setup-python</code> combined with <code>with: python-version: '3.12'</code> automatically installs and configures Python 3.12.x in the environment. I had grown accustomed to this and considered it an "out-of-the-box" feature. However, on Self-hosted Runner, the situation is somewhat different. The setup-python <a href="https://github.com/actions/setup-python/blob/main/docs/advanced-usage.md#using-setup-python-with-a-self-hosted-runner">documentation</a> states:</p>
<blockquote>
<p>Python distributions are only available for the same <a href="https://github.com/actions/runner-images#available-images">environments</a> that GitHub Actions hosted environments are available for. If you are using an unsupported version of Ubuntu such as <code>19.04</code> or another Linux distribution such as Fedora, <code>setup-python</code> may not work.</p>
</blockquote>
<p>The setup-python Action <strong>only supports the same operating systems used by GitHub Actions</strong>, and my VPS's Debian is not supported, hence this error, which also sealed Debian's fate.</p>
<h2>The Root Cause: Misunderstanding Self-hosted Runner</h2>
<p>I subconsciously believed that Self-hosted Runner merely transferred the computational cost from GitHub's servers to local infrastructure, and that official standard actions like <code>actions/setup-python</code> should elegantly download, install, and configure everything I need, just like in GitHub-hosted Runners. However, <strong>the essence of Self-hosted Runner is simply receiving tasks from GitHub and executing instructions within the current operating system environment</strong>, without guaranteeing consistency with the runtime environment provided by GitHub's official Runners.</p>
<p>Self-hosted Runner is not an out-of-the-box "service," but rather <strong>"infrastructure" that you need to personally manage</strong>. You are responsible for server installation, configuration, security updates, dependency management, disk cleanup, and a series of operational tasks. It's more suitable for teams or individuals with advanced CI/CD requirements: such as heavy CI/CD consumers, teams needing specific hardware (like ARM, GPU) for builds, or enterprises whose CI workflows deeply depend on internal network resources. For ordinary developers like me who simply want to provide more local computing resources to gain more Action runtime, the operational mental burden it brings seems a bit heavy.</p>]]>
    </content>
    <published>2025-09-04T21:54:17.000Z</published>
    <author>
      <name>竹林里有冰</name>
      <email>zhullyb@outlook.com</email>
      <uri>https://zhul.in</uri>
    </author>
    <category term="Github"></category>
    <category term="Github Action"></category>
    <category term="CI/CD"></category>
    <category term="Experience"></category>
  </entry>
  <entry>
    <id>https://zhul.in/2025/07/13/vue-markdown-render-improvement-2/</id>
    <title>Vue Markdown Rendering Optimization in Practice (Part 2): Farewell to DOM Manipulation, Embrace AST and Functional Rendering</title>
    <updated>2025-07-12T16:01:35.000Z</updated>
    <link href="https://zhul.in/2025/07/13/vue-markdown-render-improvement-2/" rel="alternate"></link>
    <content type="html">
      <![CDATA[<h2>Recap: When <code>morphdom</code> Meets Vue</h2>
<p>In <a href="/2025/07/12/vue-markdown-render-improvement-1/">the previous article</a>, we embarked on a performance optimization journey for Markdown rendering. From the most primitive full refresh with <code>v-html</code>, to block-by-block updates, we eventually brought out the "ultimate weapon" - <code>morphdom</code>. By directly comparing and manipulating the real DOM, it updates the view with minimal cost, perfectly solving the performance bottleneck and interaction state loss issues in real-time rendering.</p>
<p>However, a fundamental problem has always existed: in Vue's territory, bypassing Vue's Virtual DOM and Diff algorithm to let a third-party library directly "operate" on the real DOM always feels somewhat "unorthodox." It's like introducing an old master with a hammer and wrench for manual repairs in a precision automated factory. Although the job is done well, it always feels like it disrupts the original workflow and isn't "Vue" enough.</p>
<p>So, is there a more elegant, more "native" approach that allows us to enjoy precise updates while fully integrating with Vue's ecosystem?</p>
<p>With this question in mind, I consulted friends in frontend development groups.</p>
<blockquote>
<p>If you're building a renderer, your approach isn't the best practice. Each time you update, you generate the full virtual HTML, then optimize performance by subtracting from the HTML. However, the incremental part of each update is clear - why not directly use this incremental part for addition? The incremental part cannot be directly obtained through markdown-it, but a better approach is to transform at this step: first parse the Markdown structure, then use Vue's dynamic rendering capabilities to generate the DOM. This way, DOM reuse can leverage Vue's own abilities. — <a href="https://site.j10c.cc/">j10c</a></p>
</blockquote>
<blockquote>
<p>You can use unified with the remark-parse plugin to parse markdown strings into AST, then render based on the AST using render functions. — bii &#x26; <a href="https://github.com/nekomeowww">nekomeowww</a></p>
</blockquote>
<h2>New Approach: From "String Conversion" to "Structured Rendering"</h2>
<p>Our previous solutions, whether <code>v-html</code> or <code>morphdom</code>, shared a core approach:</p>
<p><code>Markdown String</code> -> <code>markdown-it</code> -> <code>HTML String</code> -> <code>Browser/morphdom</code> -> <code>DOM</code></p>
<p>The problem with this pipeline is that starting from the <code>HTML String</code> step, we lose the <strong>original structural information</strong> of Markdown. We get a bunch of unstructured text that Vue cannot understand its internal logic and can only swallow it whole.</p>
<p>The new approach transforms the process into:</p>
<p><code>Markdown String</code> -> <code>AST (Abstract Syntax Tree)</code> -> <code>Vue VNodes (Virtual Nodes)</code> -> <code>Vue</code> -> <code>DOM</code></p>
<h3>What is AST?</h3>
<p><strong>AST (Abstract Syntax Tree)</strong> is a structured representation of source code or markup language. It parses a long string of text into a hierarchical tree-like object. For Markdown, a level-1 heading becomes a node with <code>type: 'heading', depth: 1</code>, a paragraph becomes a node with <code>type: 'paragraph'</code>, and the text within the paragraph becomes the <code>children</code> of the <code>paragraph</code> node.</p>
<p>Once we convert Markdown into an AST, we essentially have a "structural blueprint" of the entire document. We're no longer facing a pile of ambiguous HTML strings, but rather a clear, programmable JavaScript object.</p>
<h3>Our New Tools: unified and remark</h3>
<p>To implement the <code>Markdown -> AST</code> conversion, we introduce the <code>unified</code> ecosystem.</p>
<ul>
<li><strong><a href="https://github.com/unifiedjs/unified">unified</a></strong>: A powerful content processing engine. Think of it as an assembly line where raw text is the raw material, and you process it through parsing, transformation, and serialization by adding different "plugins."</li>
<li><strong><a href="https://github.com/remarkjs/remark">remark-parse</a></strong>: A <code>unified</code> plugin specifically responsible for parsing Markdown text into AST (specifically in <a href="https://github.com/syntax-tree/mdast">mdast</a> format).</li>
</ul>
<h2>Step 1: Parse Markdown into AST</h2>
<p>First, we need to install the dependencies:</p>
<pre><code class="language-bash">npm install unified remark-parse
</code></pre>
<p>Then, we can easily convert a Markdown string into an AST:</p>
<pre><code class="language-javascript">import { unified } from 'unified'
import remarkParse from 'remark-parse'

const markdownContent = '# Hello, AST!\n\nThis is a paragraph.'

// Create a processor instance
const processor = unified().use(remarkParse)

// Parse Markdown content
const ast = processor.parse(markdownContent)

console.log(JSON.stringify(ast, null, 2))
</code></pre>
<p>Running the above code will give us a JSON object like the following, which is our coveted AST:</p>
<pre><code class="language-json">{
  "type": "root",
  "children": [
    {
      "type": "heading",
      "depth": 1,
      "children": [
        {
          "type": "text",
          "value": "Hello, AST!",
          "position": { ... }
        }
      ],
      "position": { ... }
    },
    {
      "type": "paragraph",
      "children": [
        {
          "type": "text",
          "value": "This is a paragraph.",
          "position": { ... }
        }
      ],
      "position": { ... }
    }
  ],
  "position": { ... }
}
</code></pre>
<h2>Step 2: From AST to Vue VNodes</h2>
<p>Having obtained the AST, the next step is to actually "construct" this "structural blueprint" into a user-visible interface. In Vue's world, the blueprint for describing UI is the Virtual Node (VNode), and the <code>h()</code> function (i.e., hyperscript) is the brush for creating VNodes.</p>
<p>Our task is to write a render function that can recursively traverse the AST and generate corresponding VNodes for each node type (<code>heading</code>, <code>paragraph</code>, <code>text</code>, etc.).</p>
<p>Here's a simple render function implementation:</p>
<pre><code class="language-javascript">function renderAst(node) {
  if (!node) return null
  switch (node.type) {
    case 'root':
      return h('div', {}, node.children.map(renderAst))
    case 'paragraph':
      return h('p', {}, node.children.map(renderAst))
    case 'text':
      return node.value
    case 'emphasis':
      return h('em', {}, node.children.map(renderAst))
    case 'strong':
      return h('strong', {}, node.children.map(renderAst))
    case 'inlineCode':
      return h('code', {}, node.value)
    case 'heading':
      return h('h' + node.depth, {}, node.children.map(renderAst))
    case 'code':
      return h('pre', {}, [h('code', {}, node.value)])
    case 'list':
      return h(node.ordered ? 'ol' : 'ul', {}, node.children.map(renderAst))
    case 'listItem':
      return h('li', {}, node.children.map(renderAst))
    case 'thematicBreak':
      return h('hr')
    case 'blockquote':
      return h('blockquote', {}, node.children.map(renderAst))
    case 'link':
      return h('a', { href: node.url, target: '_blank' }, node.children.map(renderAst))
    default:
      // Other unimplemented types
      return h('span', { }, `[${node.type}]`)
  }
}
</code></pre>
<h2>Step 3: Encapsulate Vue Component</h2>
<p>Integrating the above logic, we can build a Vue component. Given the characteristic of directly generating VNodes, using a functional component or explicit <code>render</code> function is most appropriate.</p>
<pre><code class="language-vue">&#x3C;template>
  &#x3C;component :is="VNodeTree" />
&#x3C;/template>

&#x3C;script setup>
import { computed, h, shallowRef, watchEffect } from 'vue'
import { unified } from 'unified'
import remarkParse from 'remark-parse'

const props = defineProps({
  mdText: {
    type: String,
    default: ''
  }
})

const ast = shallowRef(null)
const parser = unified().use(remarkParse)

watchEffect(() => {
  ast.value = parser.parse(props.mdText)
})

// AST render function (same as renderAst function above)
function renderAst(node) { ... }

const VNodeTree = computed(() => renderAst(ast.value))

&#x3C;/script>
</code></pre>
<p>Now it can be used like a regular component:</p>
<pre><code class="language-vue">&#x3C;template>
  &#x3C;MarkdownRenderer :mdText="markdownContent" />
&#x3C;/template>

&#x3C;script setup>
import { ref } from 'vue'
import MarkdownRenderer from './MarkdownRenderer.vue'

const markdownContent = ref('# Hello Vue\n\nThis is rendered via AST!')
&#x3C;/script>
</code></pre>
<h2>Huge Advantages of the AST Approach</h2>
<p>After switching to the AST track, we gained unprecedented superpowers:</p>
<ol>
<li>
<p><strong>Native Integration, Excellent Performance</strong>: We no longer need the brute force refresh of <code>v-html</code>, nor do we need "external help" like <code>morphdom</code>. All updates are handled by Vue's own Diff algorithm, which is not only highly performant but also fully aligned with Vue's design philosophy - truly "one of our own."</p>
</li>
<li>
<p><strong>High Flexibility and Extensibility</strong>: AST, as a programmable JavaScript object, provides a solid foundation for customized processing:</p>
<ul>
<li><strong>Element Replacement</strong>: Native elements (like <code>&#x3C;h2></code>) can be seamlessly replaced with custom Vue components (like <code>&#x3C;FancyHeading></code>), only requiring adjustments to the corresponding <code>case</code> logic in the <code>renderAst</code> function.</li>
<li><strong>Logic Injection</strong>: Attributes like <code>target="_blank"</code> and <code>rel="noopener noreferrer"</code> can be conveniently added to external links <code>&#x3C;a></code>, or lazy-load components can wrap images <code>&#x3C;img></code> - such operations are easy to implement at the AST level.</li>
<li><strong>Ecosystem Integration</strong>: Fully leverage <code>unified</code>'s rich plugin ecosystem (such as <code>remark-gfm</code> for GFM syntax support, <code>remark-prism</code> for code highlighting), only requiring the introduction of corresponding plugins in the processor chain (<code>.use(pluginName)</code>).</li>
</ul>
</li>
<li>
<p><strong>Separation of Concerns</strong>: Parsing logic (<code>remark</code>), rendering logic (<code>renderAst</code>), and business logic (Vue components) are clearly separated, resulting in clearer code structure and stronger maintainability.</p>
</li>
<li>
<p><strong>Type Safety and Predictability</strong>: Compared to manipulating strings or raw HTML, rendering logic based on structured AST is easier to type-check and reason about.</p>
</li>
</ol>
<h2>Conclusion: Evolution from Functional Implementation to Architectural Optimization</h2>
<p>Reviewing the optimization journey:</p>
<ul>
<li><strong>v-html</strong>: Simple implementation, but with performance and security concerns.</li>
<li><strong>Block updates</strong>: Alleviated some performance issues, but the solution had limitations.</li>
<li><strong>morphdom</strong>: Effectively improved performance and user experience, but existed in isolation from Vue's core mechanisms.</li>
<li><strong>AST + Functional Rendering</strong>: Returns to Vue's native paradigm, providing an ultimate solution with excellent performance, flexibility, and maintainability.</li>
</ul>
<p>By adopting AST, we not only solved specific technical challenges but, more importantly, achieved a paradigm shift - from result-oriented programming (HTML strings) to process and structure-oriented programming (AST). This enables us to dive deep into the essence of content, thereby achieving precise control over the rendering process.</p>
<p>This optimization practice from "full refresh" to "structured rendering" is not only a technical process of performance improvement but also a systematic exploration of deeply understanding modern frontend engineering thinking. The final Markdown rendering solution achieved high standards in performance, functionality, and architectural elegance.</p>]]>
    </content>
    <published>2025-07-12T16:01:35.000Z</published>
    <author>
      <name>竹林里有冰</name>
      <email>zhullyb@outlook.com</email>
      <uri>https://zhul.in</uri>
    </author>
    <category term="Vue.js"></category>
    <category term="Markdown"></category>
    <category term="AST"></category>
    <category term="JavaScript"></category>
    <category term="Web"></category>
    <category term="unified"></category>
  </entry>
  <entry>
    <id>https://zhul.in/2025/07/12/vue-markdown-render-improvement-1/</id>
    <title>Vue Markdown Rendering Optimization in Practice (Part 1): From Brute Force Refresh, Chunked Updates to Morphdom&apos;s Elegant Transformation</title>
    <updated>2025-07-12T12:48:56.000Z</updated>
    <link href="https://zhul.in/2025/07/12/vue-markdown-render-improvement-1/" rel="alternate"></link>
    <content type="html">
      <![CDATA[<h2>Background</h2>
<p>In a recent AI project I took over, I needed to implement a ChatGPT-like conversational interface. The core workflow is: the backend continuously pushes AI-generated Markdown text fragments to the frontend via SSE (Server-Sent Events) protocol. The frontend is responsible for dynamically receiving and concatenating these Markdown fragments, ultimately rendering and displaying the complete Markdown text in real-time on the user interface.</p>
<p>Markdown rendering isn't an uncommon requirement, especially in today's world flooded with LLM-based products. Unlike the React ecosystem, which has a popular third-party library with 14k+ stars—<a href="https://github.com/remarkjs/react-markdown">react-markdown</a>—Vue doesn't seem to have an actively maintained Markdown rendering library with significant popularity (at least 2k+ stars). <a href="https://github.com/cloudacy/vue-markdown-render#readme">cloudacy/vue-markdown-render</a> last released a year ago but has only 103 stars as of this writing; <a href="https://github.com/miaolz123/vue-markdown">miaolz123/vue-markdown</a> has 2k stars but its last commit was 7 years ago; <a href="https://github.com/zhaoxuhui1122/vue-markdown">zhaoxuhui1122/vue-markdown</a> is even archived.</p>
<h2>First Version: Simple and Brute Force v-html</h2>
<p>After a quick survey, I confirmed that the Vue ecosystem indeed lacks a robust Markdown rendering library. Since there's no ready-made solution, let's build our own!</p>
<p>Based on most articles and LLM recommendations, we first adopted the markdown-it third-party library to convert Markdown to HTML strings, then passed it through v-html.</p>
<p><strong>PS:</strong> We assume here that the Markdown content is trusted (e.g., generated by our own AI). If the content comes from user input, you must use libraries like <code>DOMPurify</code> to prevent XSS attacks and avoid "opening a window" in your website!</p>
<p>Example code:</p>
<pre><code class="language-vue">&#x3C;template>
  &#x3C;div v-html="renderedHtml">&#x3C;/div>
&#x3C;/template>

&#x3C;script setup>
import { computed, onMounted, ref } from 'vue';
import MarkdownIt from 'markdown-it';

const markdownContent = ref('');
const md = new MarkdownIt();

const renderedHtml = computed(() => md.render(markdownContent.value))

onMounted(() => {
  // markdownContent.value = await fetch() ...
})
&#x3C;/script>
</code></pre>
<h2>Evolution: Chunked Updates for Markdown</h2>
<p>While the above approach achieves basic rendering, it has obvious flaws in real-time update scenarios: <strong>every time a new Markdown fragment is received, the entire document triggers a full re-render</strong>. Even if only the last line is new content, the entire document's DOM gets completely replaced. This leads to two core problems:</p>
<ol>
<li><strong>Performance bottleneck:</strong> As Markdown content grows, the overhead of <code>markdown-it</code> parsing and DOM reconstruction increases linearly.</li>
<li><strong>Lost interaction state:</strong> Full refresh wipes out the user's current operation state. Most notably, if you've selected some text, one refresh and the selection is gone!</li>
</ol>
<p>To solve these two problems, <a href="https://juejin.cn/post/7480900772386734143">we found a chunked rendering solution online</a>—splitting Markdown by two consecutive newlines (<code>\n\n</code>) into chunks. This way, each update only re-renders the last new chunk, while previous chunks reuse the cache. The benefits are obvious:</p>
<ul>
<li>If the user has selected text in a previous chunk, the selection state won't be lost on the next update (because that chunk hasn't changed).</li>
<li>Fewer DOM nodes need re-rendering, naturally improving performance.</li>
</ul>
<p>The adjusted code looks like this:</p>
<pre><code class="language-vue">&#x3C;template>
  &#x3C;div>
    &#x3C;div
      v-for="(block, idx) in renderedBlocks"
      :key="idx"
      v-html="block"
      class="markdown-block"
    >&#x3C;/div>
  &#x3C;/div>
&#x3C;/template>

&#x3C;script setup>
import { ref, computed, watch } from 'vue'
import MarkdownIt from 'markdown-it'

const markdownContent = ref('')
const md = new MarkdownIt()

const renderedBlocks = ref([])
const blockCache = ref([])

watch(
  markdownContent,
  (newContent, oldContent) => {
    const blocks = newContent.split(/\n{2,}/)
    // Only re-render the last block, reuse cache for others
    // Handle cases where blocks decrease or increase
    blockCache.value.length = blocks.length
    for (let i = 0; i &#x3C; blocks.length; i++) {
      // Only render the last one, or new blocks
      if (i === blocks.length - 1 || !blockCache.value[i]) {
        blockCache.value[i] = md.render(blocks[i] || '')
      }
      // Reuse other blocks directly
    }
    renderedBlocks.value = blockCache.value.slice()
  },
  { immediate: true }
)

onMounted(() => {
  // markdownContent.value = await fetch() ...
})
&#x3C;/script>
</code></pre>
<h2>Ultimate Weapon: Precise Updates with morphdom</h2>
<p>While chunked rendering solves most problems, it struggles with Markdown lists. Because in Markdown syntax, list items are usually separated by only one newline, so the entire list is treated as one large chunk. Imagine a list with hundreds of items—even if only the last item is updated, the entire list chunk must be completely re-rendered, and we're back to square one.</p>
<h3>What is morphdom?</h3>
<p><code>morphdom</code> is a JavaScript library of only 5KB (gzipped), with the core functionality of: <strong>receiving two DOM nodes (or HTML strings), calculating the minimal DOM operations needed, and "morphing" the first node into the second, rather than directly replacing it</strong>.</p>
<p>Its working principle is similar to Virtual DOM's Diff algorithm, but <strong>operates directly on the real DOM</strong>:</p>
<ol>
<li>Compare tag names, attributes, text content, etc., between old and new DOM;</li>
<li>Execute only add/delete/modify operations on the differences (like modifying text, updating attributes, moving node positions);</li>
<li>Unchanged DOM nodes are completely preserved, including their event listeners, scroll positions, selection states, etc.</li>
</ol>
<p>Markdown treats lists as a whole, but in the generated HTML, each list item (<code>&#x3C;li></code>) is independent! When <code>morphdom</code> updates later list items, it ensures earlier list items remain untouched, naturally preserving their state.</p>
<p>Isn't this exactly what we've been dreaming of? Real-time Markdown updates while maximally preserving user operation state, and saving a bunch of unnecessary DOM operations!</p>
<h3>Example Code</h3>
<pre><code class="language-vue">&#x3C;template>
  &#x3C;div ref="markdownContainer" class="markdown-container">
    &#x3C;div id="md-root">&#x3C;/div>
  &#x3C;/div>
&#x3C;/template>

&#x3C;script setup>
import { nextTick, ref, watch } from 'vue';
import MarkdownIt from 'markdown-it';
import morphdom from 'morphdom';

const markdownContent = ref('');
const markdownContainer = ref(null);
const md = new MarkdownIt();

const render = () => {
  if (!markdownContainer.value.querySelector('#md-root')) return;

  const newHtml = `&#x3C;div id="md-root">` + md.render(markdownContent.value) + `&#x3C;/div>`

  morphdom(markdownContainer.value, newHtml, {
    childrenOnly: true
  });
}

watch(markdownContent, () => {
    render()
});

onMounted(async () => {
  // Wait for DOM to be mounted
  await nextTick()
  render()
})
&#x3C;/script>

</code></pre>
<h3>Seeing is Believing: Demo Comparison</h3>
<p>The iframe below contains a comparison demo showing the performance differences between different approaches.</p>
<p><strong>Tip:</strong> If you're using Chromium-based browsers like Chrome or Edge, open Developer Tools (DevTools), find the "Rendering" tab, and check "Paint flashing." This way you can visually see which parts get repainted with each update—fewer repainted areas mean better performance!</p>
<p><img src="https://static.031130.xyz/uploads/2025/07/12/d5721c40fb076.webp" alt=""></p>
<iframe src="https://static.031130.xyz/demo/morphdom-vs-markdown-chunk.html" width="100%" height="500" allowfullscreen loading="lazy"></iframe>
<h2>Progress So Far</h2>
<p>From the initial "brute force full refresh," to the "smarter chunked updates," and now to the "surgical precision of <code>morphdom</code> updates," we've progressively eliminated unnecessary rendering overhead, ultimately creating a Markdown real-time rendering solution that's both fast and preserves user state.</p>
<p>However, using <code>morphdom</code>, a third-party library to directly manipulate DOM in Vue components, feels somewhat... not quite "Vue"? While it solves the core performance and state issues, playing this way in the Vue world feels a bit like taking the back door.</p>
<p><strong>Next Episode Preview:</strong> In the next article, we'll discuss whether there's a more elegant, more "native" solution in the Vue world to achieve precise Markdown updates. Stay tuned!</p>]]>
    </content>
    <published>2025-07-12T12:48:56.000Z</published>
    <author>
      <name>竹林里有冰</name>
      <email>zhullyb@outlook.com</email>
      <uri>https://zhul.in</uri>
    </author>
    <category term="Vue.js"></category>
    <category term="Markdown"></category>
    <category term="JavaScript"></category>
    <category term="Web"></category>
    <category term="HTML"></category>
  </entry>
</feed>
