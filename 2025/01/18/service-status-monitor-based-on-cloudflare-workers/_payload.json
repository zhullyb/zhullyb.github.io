[{"data":1,"prerenderedAt":626},["ShallowReactive",2],{"post-2025-01-18-service-status-monitor-based-on-cloudflare-workers":3,"surround-2025-01-18-service-status-monitor-based-on-cloudflare-workers":615,"randomIndex/2025/01/18/service-status-monitor-based-on-cloudflare-workers/":240},{"id":4,"title":5,"body":6,"date":600,"description":116,"extension":601,"meta":602,"navigation":603,"path":604,"rawbody":605,"seo":606,"stem":607,"sticky":608,"tags":609,"__hash__":614},"posts/posts/service-status-monitor-based-on-cloudflare-workers.md","基于 Cloudflare Workers 实现的在线服务状态检测告警系统",{"type":7,"value":8,"toc":589},"minimark",[9,13,22,25,32,52,55,71,74,85,93,100,106,110,133,136,144,163,174,180,187,190,207,214,244,247,370,373,397,400,408,411,420,504,507,510,522,525,537,540,547,552,555,561,564,571,578,585],[10,11,12],"h2",{"id":12},"起因",[14,15,16,17,21],"p",{},"受一些客观因素的影响，微精弘前一阵子针对学校教务系统的数据爬取服务状态出现了非常不稳定的状态，而后端在设计初并没有考虑到异常告警机制，恰逢现任员工都身陷期末周的痛苦之中，我这种计院 Lite 专业的精弘老人就打算实现一个针对「",[18,19,20],"strong",{},"微精弘主后端 \u003C->  funnel 爬虫服务 \u003C-> 教务系统","」这一条链路的告警机制。旨在短期内（即期末周结束之前）填补微精弘的后端服务告警机制的空白，让运维人员能够及时收到并处理排查后端网络链路的异常情况，尽最大努力保证服务在线率，保障工大本科生在期末周内使用体验。",[10,23,24],{"id":24},"需求分析",[14,26,27],{},[28,29],"img",{"alt":30,"src":31},"微精弘的技术架构图","https://static.031130.xyz/uploads/2025/01/19/74362573e371d.webp",[33,34,35],"blockquote",{},[14,36,37,38,45,46,51],{},"如果你不知道微精弘的具体架构实现，这里有一篇由前技术总监提笔并由现任技术总监完善的架构杂谈「",[39,40,44],"a",{"href":41,"rel":42},"https://mp.weixin.qq.com/s/8d6JAPsLa4TzLr50uDG8uw",[43],"nofollow","微精弘 | 架构杂谈","」，原文最初发表于前者的",[39,47,50],{"href":48,"rel":49},"https://blog.cnpatrickstar.com/posts/wejh-architecture/",[43],"博客","。",[14,53,54],{},"基于上述客观条件以及我个人在服务监控告警领域近乎为 0 的经验，我一拍脑袋提出了以下几点需求：",[56,57,58,62,65,68],"ol",{},[59,60,61],"li",{},"稳定性。告警服务本身必须要比我们的主后端更加稳定，这是告警服务的基础。",[59,63,64],{},"针对现有服务的侵入性低。告警服务不能影响到现有服务，最好能够完全分离开来，不应当部署在同一台服务器上。",[59,66,67],{},"开发快速。整个服务需要尽快落地，因为现有的服务在一周内出现了三次故障，且由于主动的监控告警机制的缺失，我们每次都要等服务 down 机的两小时后才意识到服务挂掉了，如果真在考试周这个使用高峰期内再出现这样的故障不容允许的（用户需要查询考场信息）。",[59,69,70],{},"尽可能低的运维成本。没什么好解释的，谁也不希望一个告警服务占用太多的运维成本，无论是人力上的还是资源上的。",[10,72,73],{"id":73},"技术选型",[14,75,76,77,80,81,84],{},"结合我已有的经验，我选用了 ",[18,78,79],{},"Cloudflare Worker"," 来完成这个任务。Cloudflare Workers 本身是支持 ",[18,82,83],{},"cron job"," 的，能够以分钟级为单位的间隔对服务进行主动探测。Cloudflare 每天都有 10w 次免费的 Workers 调用额度，本身的服务在线率也过硬，唯一的缺点可能就是海外节点访问国内服务器的延迟过高了。不过考虑到我们探测的是在线情况而非延迟情况，倒也不是不能接受。",[14,86,87,88,92],{},"由于服务特性的关系，我们容许一定的访问失败概率。比如五次访问中如果有两三次失败，我们也认为线路是通的，",[89,90,91],"del",{},"可能只是教务系统的土豆快熟了","。因此并不是每一次失败的探测都需要进行告警。另外，我们还需要记录当前的服务状态，一旦服务被认定为下线状态，后续探测失败我们就不再进行告警，直到我们重新判定服务状态为上线（即每段连续的下线时间都只触发一次告警）。Cloudflare Worker 是一种 serverless 服务，且每次执行探测任务的可能都不是同一个 Cloudflare 的边缘节点，因此我们没法使用变量在内存中记录目前的服务状态，需要引入外部数据库来完成短期内的数据存储。",[14,94,95,96,99],{},"在数据库上，我们必须选择 Cloudflare 自家的在线数据库服务，通过 Cloudflare 自己内部的网络传输数据库查询结果才能得到尽可能低的延迟。在一番考量过后，KV 数据库和 SQL 数据库中，我果断选择了 ",[18,97,98],{},"Cloudflare D1"," 这个 SQL 数据库（本质是 SQLite），D1 数据库以更长的查询时间换取数据的实时性。Cloudflare 为免费用户提供了每天 500w 行读取和 10w 行写入的免费额度，只要好好加以利用，就不太可能超出限额。后续我还考虑通过这些数据库的数据使用 Cloudflare Worker 构建 uptime status 的后端 API，实现一个类似 status.openai.com 的在线服务状态可视化界面。",[14,101,102],{},[28,103],{"alt":104,"src":105},"OpenAI 的 uptime status","https://static.031130.xyz/uploads/2025/01/19/f817539504140.webp",[10,107,109],{"id":108},"登陆-wrangler","登陆 wrangler",[111,112,117],"pre",{"className":113,"code":114,"language":115,"meta":116,"style":116},"language-bash shiki shiki-themes github-light github-dark","wrangler login\n","bash","",[118,119,120],"code",{"__ignoreMap":116},[121,122,125,129],"span",{"class":123,"line":124},"line",1,[121,126,128],{"class":127},"sScJk","wrangler",[121,130,132],{"class":131},"sZZnC"," login\n",[10,134,135],{"id":135},"项目初始化",[14,137,138,139,143],{},"由于之前有过一些编写 Cloudflare Workers 的经验，我深知在 Cloudflare 网页的 code server 编辑器上编辑单文件的 worker.js 的不便，选择使用 Cloudflare 官方推出的工具 ",[39,140,128],{"href":141,"rel":142},"https://developers.cloudflare.com/workers/wrangler/",[43]," 进行项目的初始化。",[111,145,147],{"className":113,"code":146,"language":115,"meta":116,"style":116},"npm create cloudflare@latest wjh-monitor\n",[118,148,149],{"__ignoreMap":116},[121,150,151,154,157,160],{"class":123,"line":124},[121,152,153],{"class":127},"npm",[121,155,156],{"class":131}," create",[121,158,159],{"class":131}," cloudflare@latest",[121,161,162],{"class":131}," wjh-monitor\n",[33,164,165,168,171],{},[14,166,167],{},"Q: What would you like to start with?\nA: Hello World example",[14,169,170],{},"Q: Which template would you like to use?\nA: Hello World Worker",[14,172,173],{},"Q: Which language do you want to use?\nA: TypeScript",[14,175,176],{},[28,177],{"alt":178,"src":179},"得到的项目结构","https://static.031130.xyz/uploads/2025/01/20/bf105c9fc18a3.webp",[14,181,182,183,186],{},"我们只需要在 ",[118,184,185],{},"index.ts"," 中编写我们的主要逻辑即可。",[10,188,189],{"id":189},"数据库初始化",[111,191,193],{"className":113,"code":192,"language":115,"meta":116,"style":116},"wrangler d1 create wjh-monitor-db\n",[118,194,195],{"__ignoreMap":116},[121,196,197,199,202,204],{"class":123,"line":124},[121,198,128],{"class":127},[121,200,201],{"class":131}," d1",[121,203,156],{"class":131},[121,205,206],{"class":131}," wjh-monitor-db\n",[14,208,209,210,213],{},"随后会输出这些内容，我们需要把这些配置文件写入项目的 ",[118,211,212],{},"wrangler.toml"," 文件中",[111,215,219],{"className":216,"code":217,"language":218,"meta":116,"style":116},"language-toml shiki shiki-themes github-light github-dark","[[d1_databases]]\nbinding = \"DB\"\ndatabase_name = \"wjh-monitor-db\"\ndatabase_id = \"ffffffff-ffff-ffff-ffff-ffffffffffff\"\n","toml",[118,220,221,226,232,238],{"__ignoreMap":116},[121,222,223],{"class":123,"line":124},[121,224,225],{},"[[d1_databases]]\n",[121,227,229],{"class":123,"line":228},2,[121,230,231],{},"binding = \"DB\"\n",[121,233,235],{"class":123,"line":234},3,[121,236,237],{},"database_name = \"wjh-monitor-db\"\n",[121,239,241],{"class":123,"line":240},4,[121,242,243],{},"database_id = \"ffffffff-ffff-ffff-ffff-ffffffffffff\"\n",[14,245,246],{},"编写一个 sql 文件创建数据表，并通过 wrangler 创建它",[111,248,252],{"className":249,"code":250,"language":251,"meta":116,"style":116},"language-sql shiki shiki-themes github-light github-dark","// schema.sql\nCREATE TABLE IF NOT EXISTS DATA (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    check_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    check_item VARCHAR(10),\n    response_time INTEGER NOT NULL,\n    success BOOLEAN NOT NULL,\n    online_status BOOLEAN NOT NULL,\n    notify BOOLEAN DEFAULT FALSE,\n    other TEXT,\n    INDEX check_time_idx (check_time)\n);\nINSERT INTO DATA (\n        response_time,\n        success,\n        online_status,\n        notify,\n        other\n    )\nVALUES (100, 1, 1, 0, 'initial');\n","sql",[118,253,254,259,264,269,274,280,286,292,298,304,310,316,322,328,334,340,346,352,358,364],{"__ignoreMap":116},[121,255,256],{"class":123,"line":124},[121,257,258],{},"// schema.sql\n",[121,260,261],{"class":123,"line":228},[121,262,263],{},"CREATE TABLE IF NOT EXISTS DATA (\n",[121,265,266],{"class":123,"line":234},[121,267,268],{},"    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",[121,270,271],{"class":123,"line":240},[121,272,273],{},"    check_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",[121,275,277],{"class":123,"line":276},5,[121,278,279],{},"    check_item VARCHAR(10),\n",[121,281,283],{"class":123,"line":282},6,[121,284,285],{},"    response_time INTEGER NOT NULL,\n",[121,287,289],{"class":123,"line":288},7,[121,290,291],{},"    success BOOLEAN NOT NULL,\n",[121,293,295],{"class":123,"line":294},8,[121,296,297],{},"    online_status BOOLEAN NOT NULL,\n",[121,299,301],{"class":123,"line":300},9,[121,302,303],{},"    notify BOOLEAN DEFAULT FALSE,\n",[121,305,307],{"class":123,"line":306},10,[121,308,309],{},"    other TEXT,\n",[121,311,313],{"class":123,"line":312},11,[121,314,315],{},"    INDEX check_time_idx (check_time)\n",[121,317,319],{"class":123,"line":318},12,[121,320,321],{},");\n",[121,323,325],{"class":123,"line":324},13,[121,326,327],{},"INSERT INTO DATA (\n",[121,329,331],{"class":123,"line":330},14,[121,332,333],{},"        response_time,\n",[121,335,337],{"class":123,"line":336},15,[121,338,339],{},"        success,\n",[121,341,343],{"class":123,"line":342},16,[121,344,345],{},"        online_status,\n",[121,347,349],{"class":123,"line":348},17,[121,350,351],{},"        notify,\n",[121,353,355],{"class":123,"line":354},18,[121,356,357],{},"        other\n",[121,359,361],{"class":123,"line":360},19,[121,362,363],{},"    )\n",[121,365,367],{"class":123,"line":366},20,[121,368,369],{},"VALUES (100, 1, 1, 0, 'initial');\n",[14,371,372],{},"使用 sql 文件创建远程数据表",[111,374,376],{"className":113,"code":375,"language":115,"meta":116,"style":116},"wrangler d1 execute wjh-monitor-db --remote --file=./schema.sql\n",[118,377,378],{"__ignoreMap":116},[121,379,380,382,384,387,390,394],{"class":123,"line":124},[121,381,128],{"class":127},[121,383,201],{"class":131},[121,385,386],{"class":131}," execute",[121,388,389],{"class":131}," wjh-monitor-db",[121,391,393],{"class":392},"sj4cs"," --remote",[121,395,396],{"class":392}," --file=./schema.sql\n",[14,398,399],{},"随后我们可以使用 wrangler 在远程数据库上执行 query 命令，并获得相应的结果",[111,401,406],{"className":402,"code":404,"language":405},[403],"language-text","wrangler d1 execute wjh-monitor-db --remote --command='SELECT * FROM DATA'\n","text",[118,407,404],{"__ignoreMap":116},[10,409,410],{"id":410},"主体逻辑编写",[14,412,413,414,419],{},"说实话，代码部分没什么太多好说的，正要了解思路直接去看",[39,415,418],{"href":416,"rel":417},"https://github.com/zhullyb/wjh-monitor",[43],"源码","就好。我在这里简单提两点。",[56,421,422,432,442],{},[59,423,424,425,428,429,431],{},"目录下的 ",[118,426,427],{},"worker-configuration.d.ts"," 定义了 env 变量的类型定义，如果我们在项目中绑定了一些变量（比如我们在 ",[118,430,212],{}," 中绑定了 d1 数据库，变量名为 DB），需要在这里声明以防止后续 TypeScript 的报错。",[59,433,434,435,438,439,441],{},"在 ",[118,436,437],{},"export default {}"," 中是将会被导出的主要函数，Hello World 项目编写了 fetch 函数，这是在 workers 被通过 http 方式访问时所调用的，如果要使用 cron job 功能，我们需要编写 scheduled 函数来被 workers 调用，并在 ",[118,440,212],{}," 中配置好 crontab 触发器。",[59,443,444,445,447,448,453,454],{},"我们在 ",[118,446,212],{}," 中绑定了 DB 变量作为数据库的快捷访问方式，因此我们可以在代码中通过 ",[39,449,452],{"href":450,"rel":451},"https://developers.cloudflare.com/d1/worker-api/",[43],"D1 数据库的 Workers Binding API"," 来实现针对数据库的快捷操作，如",[111,455,459],{"className":456,"code":457,"language":458,"meta":116,"style":116},"language-javascript shiki shiki-themes github-light github-dark","const res = await env.DB.prepare(\"SELECT * FROM DATA ORDER BY check_time DESC LIMIT 4\").run();\n","javascript",[118,460,461],{"__ignoreMap":116},[121,462,463,467,470,473,476,480,483,486,489,492,495,498,501],{"class":123,"line":124},[121,464,466],{"class":465},"szBVR","const",[121,468,469],{"class":392}," res",[121,471,472],{"class":465}," =",[121,474,475],{"class":465}," await",[121,477,479],{"class":478},"sVt8B"," env.",[121,481,482],{"class":392},"DB",[121,484,485],{"class":478},".",[121,487,488],{"class":127},"prepare",[121,490,491],{"class":478},"(",[121,493,494],{"class":131},"\"SELECT * FROM DATA ORDER BY check_time DESC LIMIT 4\"",[121,496,497],{"class":478},").",[121,499,500],{"class":127},"run",[121,502,503],{"class":478},"();\n",[14,505,506],{},"需要注意的是，workers 不存在上下文，每一次访问的处理都是前后独立的，如果你需要临时存储一些数据，不要使用变量，一定要存入持久化存储的数据库。",[14,508,509],{},"预览:",[111,511,513],{"className":113,"code":512,"language":115,"meta":116,"style":116},"wrangler dev\n",[118,514,515],{"__ignoreMap":116},[121,516,517,519],{"class":123,"line":124},[121,518,128],{"class":127},[121,520,521],{"class":131}," dev\n",[14,523,524],{},"部署:",[111,526,528],{"className":113,"code":527,"language":115,"meta":116,"style":116},"wrangler deploy\n",[118,529,530],{"__ignoreMap":116},[121,531,532,534],{"class":123,"line":124},[121,533,128],{"class":127},[121,535,536],{"class":131}," deploy\n",[10,538,539],{"id":539},"小插曲",[14,541,542,543,546],{},"由于我对后端经验的缺乏，我在编写 sql 语句时没有意识到建立索引的重要性。我在查询时使用了 ",[118,544,545],{},"ORDER BY \u003C未建立索引字段> LIMIT 5"," 的方式来查询最近五次的记录，这导致数据库不得不在我每次查询时都完整遍历一遍整张表。随着 cronjob 每分钟运行时插入一条新数据，记录的行数随时间增加，每次查询的成本也逐渐增加，最终造成了单日访问八百多万行的记录，超出了 Cloudflare 的免费额度，一度造成了项目被迫下线的风险。",[14,548,549],{},[28,550],{"alt":116,"src":551},"https://static.031130.xyz/uploads/2025/01/20/fb463f45038ac.webp",[14,553,554],{},"所幸 Cloudflare 没有给我停机，而我也及时定位到了问题并补建了索引，使每日的读取量回到了正常的状态。",[14,556,557],{},[28,558],{"alt":559,"src":560},"蓝色线条为读取，黄色线条为写入","https://static.031130.xyz/uploads/2025/01/19/83f69aff3a7b4.webp",[10,562,563],{"id":563},"参见",[14,565,566],{},[39,567,570],{"href":568,"rel":569},"https://developers.cloudflare.com/d1/",[43],"Cloudflare D1 · Cloudflare D1 docs",[14,572,573],{},[39,574,577],{"href":575,"rel":576},"https://developers.cloudflare.com/workers/configuration/cron-triggers/",[43],"Cron Triggers · Cloudflare Workers docs",[14,579,580],{},[39,581,584],{"href":582,"rel":583},"https://blog.sww.moe/post/cloudflare-d1/",[43],"Cloudflare D1 使用记录",[586,587,588],"style",{},"html pre.shiki code .sScJk, html code.shiki .sScJk{--shiki-default:#6F42C1;--shiki-dark:#B392F0}html pre.shiki code .sZZnC, html code.shiki .sZZnC{--shiki-default:#032F62;--shiki-dark:#9ECBFF}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .sj4cs, html code.shiki .sj4cs{--shiki-default:#005CC5;--shiki-dark:#79B8FF}html pre.shiki code .szBVR, html code.shiki .szBVR{--shiki-default:#D73A49;--shiki-dark:#F97583}html pre.shiki code .sVt8B, html code.shiki .sVt8B{--shiki-default:#24292E;--shiki-dark:#E1E4E8}",{"title":116,"searchDepth":228,"depth":228,"links":590},[591,592,593,594,595,596,597,598,599],{"id":12,"depth":228,"text":12},{"id":24,"depth":228,"text":24},{"id":73,"depth":228,"text":73},{"id":108,"depth":228,"text":109},{"id":135,"depth":228,"text":135},{"id":189,"depth":228,"text":189},{"id":410,"depth":228,"text":410},{"id":539,"depth":228,"text":539},{"id":563,"depth":228,"text":563},"2025-01-18 02:00:08","md",{},true,"/2025/01/18/service-status-monitor-based-on-cloudflare-workers","---\ntitle: 基于 Cloudflare Workers 实现的在线服务状态检测告警系统\ndate: 2025-01-18 02:00:08\nsticky:\ntags:\n- Cloudflare\n- JavaScript\n- crontab\n- Network\n---\n\n## 起因\n\n受一些客观因素的影响，微精弘前一阵子针对学校教务系统的数据爬取服务状态出现了非常不稳定的状态，而后端在设计初并没有考虑到异常告警机制，恰逢现任员工都身陷期末周的痛苦之中，我这种计院 Lite 专业的精弘老人就打算实现一个针对「**微精弘主后端 \u003C->  funnel 爬虫服务 \u003C-> 教务系统**」这一条链路的告警机制。旨在短期内（即期末周结束之前）填补微精弘的后端服务告警机制的空白，让运维人员能够及时收到并处理排查后端网络链路的异常情况，尽最大努力保证服务在线率，保障工大本科生在期末周内使用体验。\n\n## 需求分析\n\n![微精弘的技术架构图](https://static.031130.xyz/uploads/2025/01/19/74362573e371d.webp)\n\n> 如果你不知道微精弘的具体架构实现，这里有一篇由前技术总监提笔并由现任技术总监完善的架构杂谈「[微精弘 | 架构杂谈](https://mp.weixin.qq.com/s/8d6JAPsLa4TzLr50uDG8uw)」，原文最初发表于前者的[博客](https://blog.cnpatrickstar.com/posts/wejh-architecture/)。\n\n基于上述客观条件以及我个人在服务监控告警领域近乎为 0 的经验，我一拍脑袋提出了以下几点需求：\n\n1. 稳定性。告警服务本身必须要比我们的主后端更加稳定，这是告警服务的基础。\n2. 针对现有服务的侵入性低。告警服务不能影响到现有服务，最好能够完全分离开来，不应当部署在同一台服务器上。\n3. 开发快速。整个服务需要尽快落地，因为现有的服务在一周内出现了三次故障，且由于主动的监控告警机制的缺失，我们每次都要等服务 down 机的两小时后才意识到服务挂掉了，如果真在考试周这个使用高峰期内再出现这样的故障不容允许的（用户需要查询考场信息）。\n4. 尽可能低的运维成本。没什么好解释的，谁也不希望一个告警服务占用太多的运维成本，无论是人力上的还是资源上的。\n\n## 技术选型\n\n结合我已有的经验，我选用了 **Cloudflare Worker** 来完成这个任务。Cloudflare Workers 本身是支持 **cron job** 的，能够以分钟级为单位的间隔对服务进行主动探测。Cloudflare 每天都有 10w 次免费的 Workers 调用额度，本身的服务在线率也过硬，唯一的缺点可能就是海外节点访问国内服务器的延迟过高了。不过考虑到我们探测的是在线情况而非延迟情况，倒也不是不能接受。\n\n由于服务特性的关系，我们容许一定的访问失败概率。比如五次访问中如果有两三次失败，我们也认为线路是通的，~~可能只是教务系统的土豆快熟了~~。因此并不是每一次失败的探测都需要进行告警。另外，我们还需要记录当前的服务状态，一旦服务被认定为下线状态，后续探测失败我们就不再进行告警，直到我们重新判定服务状态为上线（即每段连续的下线时间都只触发一次告警）。Cloudflare Worker 是一种 serverless 服务，且每次执行探测任务的可能都不是同一个 Cloudflare 的边缘节点，因此我们没法使用变量在内存中记录目前的服务状态，需要引入外部数据库来完成短期内的数据存储。\n\n在数据库上，我们必须选择 Cloudflare 自家的在线数据库服务，通过 Cloudflare 自己内部的网络传输数据库查询结果才能得到尽可能低的延迟。在一番考量过后，KV 数据库和 SQL 数据库中，我果断选择了 **Cloudflare D1** 这个 SQL 数据库（本质是 SQLite），D1 数据库以更长的查询时间换取数据的实时性。Cloudflare 为免费用户提供了每天 500w 行读取和 10w 行写入的免费额度，只要好好加以利用，就不太可能超出限额。后续我还考虑通过这些数据库的数据使用 Cloudflare Worker 构建 uptime status 的后端 API，实现一个类似 status.openai.com 的在线服务状态可视化界面。\n\n![OpenAI 的 uptime status](https://static.031130.xyz/uploads/2025/01/19/f817539504140.webp)\n\n## 登陆 wrangler\n\n```bash\nwrangler login\n```\n\n## 项目初始化\n\n由于之前有过一些编写 Cloudflare Workers 的经验，我深知在 Cloudflare 网页的 code server 编辑器上编辑单文件的 worker.js 的不便，选择使用 Cloudflare 官方推出的工具 [wrangler](https://developers.cloudflare.com/workers/wrangler/) 进行项目的初始化。\n\n```bash\nnpm create cloudflare@latest wjh-monitor\n```\n\n> Q: What would you like to start with? \n> A: Hello World example \n>\n> Q: Which template would you like to use? \n> A: Hello World Worker \n>\n> Q: Which language do you want to use? \n> A: TypeScript\n\n![得到的项目结构](https://static.031130.xyz/uploads/2025/01/20/bf105c9fc18a3.webp)\n\n我们只需要在 `index.ts` 中编写我们的主要逻辑即可。\n\n## 数据库初始化\n\n```bash\nwrangler d1 create wjh-monitor-db\n```\n\n随后会输出这些内容，我们需要把这些配置文件写入项目的 `wrangler.toml` 文件中\n\n```toml\n[[d1_databases]]\nbinding = \"DB\"\ndatabase_name = \"wjh-monitor-db\"\ndatabase_id = \"ffffffff-ffff-ffff-ffff-ffffffffffff\"\n```\n\n编写一个 sql 文件创建数据表，并通过 wrangler 创建它\n\n```sql\n// schema.sql\nCREATE TABLE IF NOT EXISTS DATA (\n\tid INTEGER PRIMARY KEY AUTOINCREMENT,\n\tcheck_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n\tcheck_item VARCHAR(10),\n\tresponse_time INTEGER NOT NULL,\n\tsuccess BOOLEAN NOT NULL,\n\tonline_status BOOLEAN NOT NULL,\n\tnotify BOOLEAN DEFAULT FALSE,\n\tother TEXT,\n\tINDEX check_time_idx (check_time)\n);\nINSERT INTO DATA (\n\t\tresponse_time,\n\t\tsuccess,\n\t\tonline_status,\n\t\tnotify,\n\t\tother\n\t)\nVALUES (100, 1, 1, 0, 'initial');\n```\n\n使用 sql 文件创建远程数据表\n\n```bash\nwrangler d1 execute wjh-monitor-db --remote --file=./schema.sql\n```\n\n随后我们可以使用 wrangler 在远程数据库上执行 query 命令，并获得相应的结果\n\n```\nwrangler d1 execute wjh-monitor-db --remote --command='SELECT * FROM DATA'\n```\n\n## 主体逻辑编写\n\n说实话，代码部分没什么太多好说的，正要了解思路直接去看[源码](https://github.com/zhullyb/wjh-monitor)就好。我在这里简单提两点。\n\n1. 目录下的 `worker-configuration.d.ts` 定义了 env 变量的类型定义，如果我们在项目中绑定了一些变量（比如我们在 `wrangler.toml` 中绑定了 d1 数据库，变量名为 DB），需要在这里声明以防止后续 TypeScript 的报错。\n\n2. 在 `export default {}` 中是将会被导出的主要函数，Hello World 项目编写了 fetch 函数，这是在 workers 被通过 http 方式访问时所调用的，如果要使用 cron job 功能，我们需要编写 scheduled 函数来被 workers 调用，并在 `wrangler.toml` 中配置好 crontab 触发器。\n\n3. 我们在 `wrangler.toml` 中绑定了 DB 变量作为数据库的快捷访问方式，因此我们可以在代码中通过 [D1 数据库的 Workers Binding API](https://developers.cloudflare.com/d1/worker-api/) 来实现针对数据库的快捷操作，如\n\n   ```javascript\n   const res = await env.DB.prepare(\"SELECT * FROM DATA ORDER BY check_time DESC LIMIT 4\").run();\n   ```\n\n需要注意的是，workers 不存在上下文，每一次访问的处理都是前后独立的，如果你需要临时存储一些数据，不要使用变量，一定要存入持久化存储的数据库。\n\n预览:\n\n```bash\nwrangler dev\n```\n\n部署:\n\n```bash\nwrangler deploy\n```\n\n## 小插曲\n\n由于我对后端经验的缺乏，我在编写 sql 语句时没有意识到建立索引的重要性。我在查询时使用了 `ORDER BY \u003C未建立索引字段> LIMIT 5` 的方式来查询最近五次的记录，这导致数据库不得不在我每次查询时都完整遍历一遍整张表。随着 cronjob 每分钟运行时插入一条新数据，记录的行数随时间增加，每次查询的成本也逐渐增加，最终造成了单日访问八百多万行的记录，超出了 Cloudflare 的免费额度，一度造成了项目被迫下线的风险。\n\n![](https://static.031130.xyz/uploads/2025/01/20/fb463f45038ac.webp)\n\n所幸 Cloudflare 没有给我停机，而我也及时定位到了问题并补建了索引，使每日的读取量回到了正常的状态。\n\n![蓝色线条为读取，黄色线条为写入](https://static.031130.xyz/uploads/2025/01/19/83f69aff3a7b4.webp)\n\n## 参见\n\n[Cloudflare D1 · Cloudflare D1 docs](https://developers.cloudflare.com/d1/)\n\n[Cron Triggers · Cloudflare Workers docs](https://developers.cloudflare.com/workers/configuration/cron-triggers/)\n\n[Cloudflare D1 使用记录](https://blog.sww.moe/post/cloudflare-d1/)\n",{"title":5,"description":116},"posts/service-status-monitor-based-on-cloudflare-workers",false,[610,611,612,613],"Cloudflare","JavaScript","crontab","Network","w7Zr5rU5F4jtCA0dovn1FnHaGYlu3E_tM6yT3PylTsw",[616,621],{"title":617,"path":618,"stem":619,"date":620,"children":-1},"使用 Cloudflare Workers 监控 Fedora Copr 构建状态","/2025/02/23/monitor-copr-build-state-with-cloudflare-workers","posts/monitor-copr-build-state-with-cloudflare-workers","2025-02-23 12:12:53",{"title":622,"path":623,"stem":624,"date":625,"children":-1},"构建部署在 Cloudflare Workers 上的 TG Bot","/2024/12/30/tg-bot-hosted-on-cloudflare-workers","posts/tg-bot-hosted-on-cloudflare-workers","2024-12-30 19:45:43",1761735720712]